{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a344f614",
      "metadata": {},
      "source": [
        "# Huggett (1993) Model with Aggregate Risk\n",
        "\n",
        "Heterogeneous-agent economy: individuals have uninsured idiosyncratic labor income risk and save in bonds; bonds are in **zero net supply**. Aggregate TFP $z_t$ is stochastic. The interest rate $r_t$ clears the bond market.\n",
        "\n",
        "**Individual state:** $(b, y)$ — bond holdings and idiosyncratic income.  \n",
        "**Aggregate state:** $z$ (and in equilibrium, the cross-sectional distribution).  \n",
        "**Budget:** $c + b' = (1+r)b + y\\,z$, with borrowing constraint $b \\geq \\underline{b}$.\n",
        "\n",
        "We solve for equilibrium using **SRL/SPG** (structural policy gradient): policy $\\pi(b,y,r,z) \\to (c,b')$; price $r = P^*(G,z)$ from market clearing with **gradient-stop** so agents take prices as given."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ed37fc0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "np.random.seed(42)\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['axes.unicode_minus'] = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45e1e82d",
      "metadata": {},
      "source": [
        "## Global parameters (SRL Appendix A.1 — Table 2 & Section 4.1)\n",
        "\n",
        "- **Preferences:** $E_0 \\sum_{t=0}^\\infty \\beta^t u(c_t)$, isoelastic $u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}$.\n",
        "- **Idiosyncratic income** $y_t$: log AR(1), persistence $\\rho_y$, innovation volatility $\\nu_y$; discretized (e.g. Tauchen) on $n_y$ points.\n",
        "- **Aggregate income** $z_t$: log AR(1), persistence $\\rho_z$, volatility $\\nu_z$; discretized (Tauchen) on $n_z$ points.\n",
        "- **Bonds:** borrowing limit $\\underline{b}$, total bond supply $B = 0$ (zero net supply)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f0303667",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Huggett calibration (SRL Appendix A.1):\n",
            "  β=0.96, σ=2.0, ρy=0.6, νy=0.2, ρz=0.9, νz=0.02\n",
            "  B=0.0, b_min=-1.0\n",
            "  Grids: nb=200, b_max=50.0, ny=3, nr=20, r∈[0.01,0.06], nz=30\n",
            "  T_trunc=170, c_min=0.001\n"
          ]
        }
      ],
      "source": [
        "# === Calibration (Table 2: Huggett model calibration) ===\n",
        "# We interpret one period as a year (Section 4.1).\n",
        "\n",
        "# Preferences\n",
        "beta = 0.96       # discount factor\n",
        "sigma = 2.0       # coefficient of relative risk aversion (CRRA)\n",
        "\n",
        "# Idiosyncratic labor income: log AR(1) with persistence ρy, innovation volatility νy\n",
        "rho_y = 0.6\n",
        "nu_y = 0.2\n",
        "\n",
        "# Aggregate TFP z: log AR(1)\n",
        "rho_z = 0.9\n",
        "nu_z = 0.02\n",
        "\n",
        "# Bonds\n",
        "B = 0.0          # total bond supply (zero net supply)\n",
        "b_min = -1.0     # borrowing constraint (b ≥ b_min)\n",
        "\n",
        "# === Discretization (Table 3: hyperparameters) ===\n",
        "nb = 200         # number of bond grid points\n",
        "b_max = 50.0     # upper bound of bond grid\n",
        "ny = 3           # number of y grid points (idiosyncratic income)\n",
        "nr = 20          # number of r grid points (market clearing)\n",
        "r_min = 0.01     # lower bound of r grid [r_L, r_H]\n",
        "r_max = 0.06     # upper bound of r grid\n",
        "nz = 30          # number of z grid points (aggregate TFP)\n",
        "\n",
        "# Truncation and numerics (Appendix A.1)\n",
        "c_min = 1e-3     # minimum consumption floor (avoids u(c) at 0)\n",
        "etrunc = 1e-3    # truncation tolerance: β^T < etrunc\n",
        "T_trunc = int(np.ceil(np.log(etrunc) / np.log(beta)))  # ≈ 170 for beta=0.96\n",
        "\n",
        "print(\"Huggett calibration (SRL Appendix A.1):\")\n",
        "print(f\"  β={beta}, σ={sigma}, ρy={rho_y}, νy={nu_y}, ρz={rho_z}, νz={nu_z}\")\n",
        "print(f\"  B={B}, b_min={b_min}\")\n",
        "print(f\"  Grids: nb={nb}, b_max={b_max}, ny={ny}, nr={nr}, r∈[{r_min},{r_max}], nz={nz}\")\n",
        "print(f\"  T_trunc={T_trunc}, c_min={c_min}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccc7ad1c",
      "metadata": {},
      "source": [
        "## Grids and utility\n",
        "\n",
        "- **Bond grid:** equispaced or log-spaced on $[\\underline{b},\\, b_{\\max}]$.\n",
        "- **Income grids:** Tauchen discretization of the log AR(1) processes for $y$ and $z$ (to be implemented)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d93b70f6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b_grid: -1.0 ... 50.0 shape (200,)\n"
          ]
        }
      ],
      "source": [
        "# Bond grid: nb points on [b_min, b_max] (Appendix: nb=200, bmax=50)\n",
        "b_grid = np.linspace(b_min, b_max, nb)\n",
        "\n",
        "# CRRA utility and derivative (for FOC / Euler)\n",
        "def u(c, sig=sigma):\n",
        "    \"\"\"u(c) = c^(1-σ)/(1-σ); limit for σ=1 is log(c).\"\"\"\n",
        "    c = np.maximum(c, c_min)\n",
        "    if np.abs(sig - 1.0) < 1e-10:\n",
        "        return np.log(c)\n",
        "    return (c ** (1 - sig)) / (1 - sig)\n",
        "\n",
        "def u_prime(c, sig=sigma):\n",
        "    \"\"\"u'(c) = c^(-σ).\"\"\"\n",
        "    c = np.maximum(c, c_min)\n",
        "    return c ** (-sig)\n",
        "\n",
        "# Placeholder: y and z grids will be built via Tauchen (same as in SRL)\n",
        "# y_grid: ny points; z_grid: nz points; transition matrices Ty, Tz\n",
        "print(\"b_grid:\", b_grid[0], \"...\", b_grid[-1], \"shape\", b_grid.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e1542106",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y_grid (idiosyncratic, AR(1), ny=3): [0.4528015  0.95858078 2.02931553]\n",
            "Ty (idiosyncratic transition):\n",
            " [[6.46169767e-01 3.53811697e-01 1.85367378e-05]\n",
            " [3.03963618e-02 9.39207276e-01 3.03963618e-02]\n",
            " [1.85367378e-05 3.53811697e-01 6.46169767e-01]]\n",
            "z_grid (aggregate, first 5 ... last 2): [0.87048321 0.87878612 0.88716821 0.89563026 0.90417302] ... [1.13552944 1.14636043]\n"
          ]
        }
      ],
      "source": [
        "# return the grid and the transition matrix\n",
        "def tauchen_ar1(rho, sigma_innov, n_states, m=3):\n",
        "    \"\"\"\n",
        "    Tauchen (1986): discretize AR(1) x' = rho*x + eps, eps ~ N(0, sigma_innov^2).\n",
        "    Returns grid (n_states,) and transition matrix (n_states, n_states).\n",
        "    m: number of std devs for the state grid (wider grid for persistence).\n",
        "    \"\"\"\n",
        "    sigma_x = sigma_innov / np.sqrt(1 - rho**2)\n",
        "    x_min = -m * sigma_x\n",
        "    x_max = m * sigma_x\n",
        "    x_grid = np.linspace(x_min, x_max, n_states)\n",
        "    step = (x_max - x_min) / (n_states - 1)\n",
        "    P = np.zeros((n_states, n_states))\n",
        "    # the way to discretize is by integrating over the interval length of step and centraled by the value of x_grid\n",
        "    for i in range(n_states):\n",
        "        for j in range(n_states):\n",
        "            if j == 0:\n",
        "                P[i, j] = norm.cdf((x_grid[j] - rho * x_grid[i] + step / 2) / sigma_innov)\n",
        "            elif j == n_states - 1:\n",
        "                P[i, j] = 1 - norm.cdf((x_grid[j] - rho * x_grid[i] - step / 2) / sigma_innov)\n",
        "            else:\n",
        "                P[i, j] = (norm.cdf((x_grid[j] - rho * x_grid[i] + step / 2) / sigma_innov) -\n",
        "                           norm.cdf((x_grid[j] - rho * x_grid[i] - step / 2) / sigma_innov))\n",
        "    P = P / P.sum(axis=1, keepdims=True)\n",
        "    return x_grid, P\n",
        "\n",
        "# Idiosyncratic income y: log y follows AR(1) with rho_y, nu_y (Table 2). Grid in logs then exp.\n",
        "log_y_grid, Ty = tauchen_ar1(rho_y, nu_y, ny)\n",
        "y_grid = np.exp(log_y_grid)\n",
        "# Normalize so E[y]=1 in stationary distribution\n",
        "invariant_y = np.linalg.matrix_power(Ty.T, 200)[:, 0]\n",
        "y_grid = y_grid / (y_grid @ invariant_y)\n",
        "\n",
        "# Aggregate z: log z follows AR(1) with rho_z, nu_z\n",
        "log_z_grid, Tz = tauchen_ar1(rho_z, nu_z, nz)\n",
        "z_grid = np.exp(log_z_grid)\n",
        "invariant_z = np.linalg.matrix_power(Tz.T, 200)[:, 0]\n",
        "z_grid = z_grid / (z_grid @ invariant_z)\n",
        "\n",
        "print(\"y_grid (idiosyncratic, AR(1), ny=%d):\" % ny, y_grid)\n",
        "print(\"Ty (idiosyncratic transition):\\n\", Ty)\n",
        "print(\"z_grid (aggregate, first 5 ... last 2):\", z_grid[:5], \"...\", z_grid[-2:])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "156e95f2",
      "metadata": {},
      "source": [
        "## Policy Functions\n",
        "\n",
        "Policy functions for getting consumption strategy π(b, y, r, z). The actual training uses Monte Carlo simulation inside `spg_objective` (see SRL section below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fef86403",
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.interpolate import interp2d, LinearNDInterpolator\n",
        "from scipy.optimize import brentq\n",
        "\n",
        "def policy_from_grid(b, y, r, z, theta_grid, b_grid, y_grid, z_grid, r_grid, ny, c_min_val=1e-3):\n",
        "    \"\"\"\n",
        "    Policy π(b, y, r, z) from trained grid parameters θ.\n",
        "    Consumption c = π(b, y, r, z) is obtained by interpolation from theta_grid.\n",
        "    theta_grid shape: (J, nz, nr) where J = nb * ny\n",
        "    Returns (c, b_next) where b_next = (1+r)*b + y*z - c; b_next is placed on b_grid via Young's lottery method.\n",
        "    \"\"\"\n",
        "    # Convert to numpy if needed\n",
        "    if hasattr(theta_grid, 'detach'):\n",
        "        theta_grid = theta_grid.detach().cpu().numpy()\n",
        "    if hasattr(b_grid, 'detach'):\n",
        "        b_grid = b_grid.detach().cpu().numpy()\n",
        "    if hasattr(y_grid, 'detach'):\n",
        "        y_grid = y_grid.detach().cpu().numpy()\n",
        "    if hasattr(z_grid, 'detach'):\n",
        "        z_grid = z_grid.detach().cpu().numpy()\n",
        "    if hasattr(r_grid, 'detach'):\n",
        "        r_grid = r_grid.detach().cpu().numpy()\n",
        "    \n",
        "    # Inputs (b, y, z, r) are assumed to be on the respective grids; b' is put on grid by Young's method below.\n",
        "    ib = np.searchsorted(b_grid, b, side='right') - 1\n",
        "    iy = np.searchsorted(y_grid, y, side='right') - 1\n",
        "    iz = np.searchsorted(z_grid, z, side='right') - 1\n",
        "    ir = np.searchsorted(r_grid, r, side='right') - 1\n",
        "    \n",
        "    # Handle vectorized inputs\n",
        "    if np.isscalar(b):\n",
        "        ib = [ib]\n",
        "        iy = [iy]\n",
        "        iz = [iz]\n",
        "        ir = [ir]\n",
        "        b = np.array([b])\n",
        "        y = np.array([y])\n",
        "        r = np.array([r])\n",
        "        z = np.array([z])\n",
        "    \n",
        "    # Get consumption from grid: j = ib * ny + iy\n",
        "    # theta_grid[j, iz, ir] directly stores consumption c = π(b, y, r, z)\n",
        "    c = np.zeros(len(b))\n",
        "    for i in range(len(b)):\n",
        "        j = ib[i] * ny + iy[i]\n",
        "        # Get consumption from trained grid\n",
        "        c[i] = np.maximum(theta_grid[j, iz[i], ir[i]], c_min_val)\n",
        "    \n",
        "    # Compute b_next from budget constraint: b' = (1+r)*b + y*z - c\n",
        "    c_total = (1 + r) * b + y * z  # Total resources available\n",
        "    b_next = c_total - c  # Remaining after consumption\n",
        "    # Clamp to [b_min, b_max] for feasibility and adjust c to satisfy budget\n",
        "    b_next_feasible = np.clip(b_next, b_min, b_max)\n",
        "    c = np.maximum(c_total - b_next_feasible, c_min_val)\n",
        "    # Young's lottery method: disperse b' onto the grid (lottery between two bracketing grid points)\n",
        "    i_lo = np.clip(np.searchsorted(b_grid, b_next_feasible, side='right') - 1, 0, len(b_grid) - 2)\n",
        "    gap = b_grid[i_lo + 1] - b_grid[i_lo]\n",
        "    lam = np.clip((b_next_feasible - b_grid[i_lo]) / (gap + 1e-12), 0.0, 1.0)\n",
        "    lottery = np.random.rand(len(b_next_feasible)) < lam\n",
        "    b_next = np.where(lottery, b_grid[i_lo + 1], b_grid[i_lo])\n",
        "    \n",
        "    if len(c) == 1:\n",
        "        return c[0], b_next[0]\n",
        "    return c, b_next\n",
        "\n",
        "\n",
        "def draw_next_state(y_idx, z_idx, Ty, Tz):\n",
        "    \"\"\"Draw (y_{t+1}, z_{t+1}) indices according to transition matrices Ty, Tz.\"\"\"\n",
        "    # P(y_{t+1}=j | y_t=y_idx) = Ty[y_idx, j]\n",
        "    y_next = np.random.choice(Ty.shape[1], p=Ty[y_idx, :])\n",
        "    # P(z_{t+1}=k | z_t=z_idx) = Tz[z_idx, k]\n",
        "    z_next = np.random.choice(Tz.shape[1], p=Tz[z_idx, :])\n",
        "    return y_next, z_next"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "05faaceb",
      "metadata": {},
      "outputs": [],
      "source": [
        "def aggregate_saving(G, r, z, policy_fn, b_grid, y_grid):\n",
        "    \"\"\"Aggregate saving = sum_j G[j] * E[b'(j)] (expected b' under policy). G = distribution on (b_grid, y_grid).\"\"\"\n",
        "    G = np.asarray(G).ravel()\n",
        "    nb, ny = len(b_grid), len(y_grid)\n",
        "    b_flat = np.repeat(b_grid, ny)\n",
        "    y_flat = np.tile(y_grid, nb)\n",
        "    c = policy_fn(b_flat, y_flat, r, z)[0]\n",
        "    b_next_expected = np.clip((1 + r) * b_flat + y_flat * z - c, b_min, b_max)\n",
        "    return np.dot(G, b_next_expected)\n",
        "\n",
        "def ge_price(G, z, policy_fn, b_grid, y_grid, r_lo=None, r_hi=None):\n",
        "    \"\"\"\n",
        "    P*(G_t, z_t): find r_t such that aggregate saving = B (market clearing).\n",
        "    In Huggett, B=0 so we solve sum b'*G = 0.\n",
        "    \"\"\"\n",
        "    r_lo = r_min if r_lo is None else r_lo\n",
        "    r_hi = r_max if r_hi is None else r_hi\n",
        "    def excess_saving(r):\n",
        "        return aggregate_saving(G, r, z, policy_fn, b_grid, y_grid) - B\n",
        "    if excess_saving(r_lo) * excess_saving(r_hi) > 0:\n",
        "        return (r_lo + r_hi) / 2.0  # fallback if no crossing\n",
        "    return brentq(excess_saving, r_lo, r_hi)\n",
        "\n",
        "def build_A_numpy(r, z, policy_fn, b_grid, y_grid, Ty):\n",
        "    \"\"\"Transition matrix A: A[j',j] = P(s'=j'|s=j) under policy at (r,z). Young: mass (1-λ) on i_lo, λ on i_lo+1.\"\"\"\n",
        "    nb, ny = len(b_grid), len(y_grid)\n",
        "    J = nb * ny\n",
        "    b_flat = np.repeat(b_grid, ny)\n",
        "    y_flat = np.tile(y_grid, nb)\n",
        "    c, _ = policy_fn(b_flat, y_flat, r, z)\n",
        "    b_next_feasible = np.clip((1 + r) * b_flat + y_flat * z - c, b_min, b_max)\n",
        "    A = np.zeros((J, J))\n",
        "    for j in range(J):\n",
        "        ib, iy = j // ny, j % ny\n",
        "        b_next_j = b_next_feasible[j]\n",
        "        i_lo = np.clip(np.searchsorted(b_grid, b_next_j, side='right') - 1, 0, nb - 2)\n",
        "        gap = b_grid[i_lo + 1] - b_grid[i_lo]\n",
        "        lam = np.clip((b_next_j - b_grid[i_lo]) / (gap + 1e-12), 0.0, 1.0)\n",
        "        for iyp in range(ny):\n",
        "            A[i_lo * ny + iyp, j] += (1 - lam) * Ty[iy, iyp]\n",
        "            A[(i_lo + 1) * ny + iyp, j] += lam * Ty[iy, iyp]\n",
        "    return A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3dcc075a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_huggett(\n",
        "    T,\n",
        "    policy_fn,\n",
        "    b_grid, y_grid, z_grid, Ty, Tz,\n",
        "    G0=None,\n",
        "    z0_idx=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Simulate the Huggett economy for T periods by maintaining the distribution G (no agents).\n",
        "    GE: r_t clears bond market each period. G_{t+1} = A_π(r_t, z_t) @ G_t.\n",
        "\n",
        "    Returns:\n",
        "        paths: dict with 'G', 'z_idx', 'z', 'r', 'c_grid'\n",
        "        v_hat: scalar, sum_t β^t * G_t^T @ u(c_t)\n",
        "    \"\"\"\n",
        "    nb, ny = len(b_grid), len(y_grid)\n",
        "    J = nb * ny\n",
        "    if G0 is None:\n",
        "        G0 = np.ones(J) / J\n",
        "    G = np.asarray(G0).ravel()\n",
        "    if z0_idx is None:\n",
        "        z0_idx = np.random.randint(0, len(z_grid))\n",
        "    z_idx = z0_idx\n",
        "\n",
        "    paths = {\n",
        "        'G': np.zeros((T + 1, J)),\n",
        "        'z_idx': np.zeros(T + 1, dtype=int),\n",
        "        'z': np.zeros(T + 1),\n",
        "        'r': np.zeros(T + 1),\n",
        "        'c_grid': np.zeros((T + 1, J)),\n",
        "    }\n",
        "    paths['G'][0] = G\n",
        "    paths['z_idx'][0] = z_idx\n",
        "    paths['z'][0] = z_grid[z_idx]\n",
        "    paths['r'][0] = np.nan\n",
        "    paths['c_grid'][0] = np.nan\n",
        "\n",
        "    b_flat = np.repeat(b_grid, ny)\n",
        "    y_flat = np.tile(y_grid, nb)\n",
        "    v_hat = 0.0\n",
        "    for t in range(T):\n",
        "        z_val = z_grid[z_idx]\n",
        "        r_t = ge_price(G, z_val, policy_fn, b_grid, y_grid)\n",
        "        paths['r'][t] = r_t\n",
        "        c_t = policy_fn(b_flat, y_flat, r_t, z_val)[0]\n",
        "        paths['c_grid'][t] = c_t\n",
        "        v_hat += (beta ** t) * np.dot(G, u(c_t))\n",
        "        A_t = build_A_numpy(r_t, z_val, policy_fn, b_grid, y_grid, Ty)\n",
        "        G = A_t @ G\n",
        "        paths['G'][t + 1] = G\n",
        "        z_idx = np.random.choice(len(z_grid), p=Tz[z_idx, :])\n",
        "        paths['z_idx'][t + 1] = z_idx\n",
        "        paths['z'][t + 1] = z_grid[z_idx]\n",
        "    paths['r'][T] = paths['r'][T - 1]\n",
        "    paths['c_grid'][T] = policy_fn(b_flat, y_flat, paths['r'][T], paths['z'][T])[0]\n",
        "\n",
        "    return paths, v_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "43274d3b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note: For SRL training, Monte Carlo simulation is done inside spg_objective().\n",
        "# The simulate_huggett() function above is kept for reference/testing purposes only.\n",
        "# SRL training uses Monte Carlo trajectories directly in the objective function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8dd29c23",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization code for simulation results (optional, for testing purposes)\n",
        "# For SRL, visualization can be done after training using the trained policy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec0d3160",
      "metadata": {},
      "source": [
        "## SRL / SPG: Gradient-stop on macro, gradient descent on policy (SRL Section 3.2–3.3)\n",
        "\n",
        "**Objective:** Maximize expected lifetime utility\n",
        "\n",
        "$$L(\\theta) = d_0^T \\hat{v}_\\pi$$\n",
        "\n",
        "where $\\hat{v}_\\pi$ is the sample average over $N$ simulated trajectories (Monte Carlo).\n",
        "\n",
        "**Macro (no gradient):** The price $p_t = P^*(g_t, z_t)$ is given by market clearing. We apply **stop-gradient** to $p_t$, so $\\partial p_t / \\partial \\theta = 0$ — agents take prices as given.\n",
        "\n",
        "**Micro (differentiate):** The policy $\\pi(\\cdot; \\theta)$ and the transition matrix $A_\\pi(z, p)$ depend on $\\theta$; we backpropagate through them and through $u(c)$.\n",
        "\n",
        "**Update:** Stochastic gradient ascent\n",
        "\n",
        "$$\\theta_{k+1} = \\theta_k + \\eta_k \\nabla_\\theta L(\\theta_k)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "90512fb0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# SRL/SPG: PyTorch for autograd; gradient-stop on prices\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "dtype = torch.float32\n",
        "\n",
        "# Coarser grid for SPG (Appendix Table 3: nb=200, but we use smaller for speed)\n",
        "nb_spg = 50\n",
        "nr_spg = 10\n",
        "nz_spg = 10\n",
        "# ny, b_min, b_max, etc. from above; use same y_grid, z_grid, Ty, Tz (or subsample)\n",
        "J = nb_spg * ny  # individual states: (b_idx, y_idx) -> j = b_idx*ny + y_idx\n",
        "\n",
        "# Build SPG grids (can subsample from main grids)\n",
        "b_grid_spg = torch.tensor(np.linspace(b_min, b_max, nb_spg), dtype=dtype, device=device)\n",
        "# Subsample z and r for SPG\n",
        "iz_spg = np.linspace(0, nz-1, nz_spg, dtype=int)\n",
        "ir_spg = np.linspace(0, nr-1, nr_spg, dtype=int) if nr <= 20 else np.arange(nr_spg)\n",
        "z_grid_t = torch.tensor(z_grid[iz_spg], dtype=dtype, device=device)\n",
        "r_grid_t = torch.tensor(np.linspace(r_min, r_max, nr_spg), dtype=dtype, device=device)\n",
        "y_grid_t = torch.tensor(y_grid, dtype=dtype, device=device)\n",
        "Ty_t = torch.tensor(Ty, dtype=dtype, device=device)\n",
        "Tz_sub = Tz[np.ix_(iz_spg, iz_spg)] if len(iz_spg) <= len(z_grid) else Tz\n",
        "Tz_sub = Tz_sub / Tz_sub.sum(axis=1, keepdims=True)\n",
        "Tz_t = torch.tensor(Tz_sub, dtype=dtype, device=device)\n",
        "nz_spg = Tz_t.shape[0]\n",
        "\n",
        "# Policy parameter θ: consumption c on (J, nz_spg, nr_spg) grid. Initialize near steady-state consumption.\n",
        "# c(s,z,p) = θ[j, iz, ir]. Budget: b' = (1+r)*b + y*z - c, clamped.\n",
        "def theta_to_consumption_grid(theta, b_grid_t, y_grid_t, z_grid_t, r_grid_t, c_min_val=1e-3):\n",
        "    \"\"\"θ shape (J, nz, nr) -> c same shape, ensured >= c_min.\"\"\"\n",
        "    return torch.nn.functional.softplus(theta) + c_min_val\n",
        "\n",
        "# Initial θ: imply consumption from a constant saving rule (e.g. c = (1-ss)*c)\n",
        "def init_theta(b_grid_t, y_grid_t, z_grid_t, r_grid_t, save_frac=0.2, c_min_val=1e-3):\n",
        "    J = len(b_grid_t) * len(y_grid_t)\n",
        "    nz_t, nr_t = len(z_grid_t), len(r_grid_t)\n",
        "    c_grid = torch.zeros(J, nz_t, nr_t, dtype=dtype, device=device)\n",
        "    for ib in range(len(b_grid_t)):\n",
        "        for iy in range(len(y_grid_t)):\n",
        "            j = ib * len(y_grid_t) + iy\n",
        "            b = b_grid_t[ib].item()\n",
        "            y = y_grid_t[iy].item()\n",
        "            for iz in range(nz_t):\n",
        "                z = z_grid_t[iz].item()\n",
        "                for ir in range(nr_t):\n",
        "                    r = r_grid_t[ir].item()\n",
        "                    c = (1 + r) * b + y * z\n",
        "                    c = max((1 - save_frac) * c, c_min_val)\n",
        "                    c_grid[j, iz, ir] = c\n",
        "    # inverse of softplus: theta such that softplus(theta)+c_min = c_grid\n",
        "    x = c_grid - c_min_val\n",
        "    theta_init = torch.log(torch.exp(x) - 1 + 1e-8)\n",
        "    return theta_init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e2729ccf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build transition matrix A_π(z, p) from θ. A_π is J×J; differentiable in θ. Use detached p for macro.\n",
        "def build_A_pi(theta, iz, ir, b_grid_t, y_grid_t, z_grid_t, r_grid_t, Ty_t, nb_spg, ny, sigma_b=0.1):\n",
        "    \"\"\"\n",
        "    A_π[j', j] = P(s'=j' | s=j) under policy θ at (z_grid[iz], r_grid[ir]).\n",
        "    b' = (1+r)*b + y*z - c; map b' to b-grid via soft weights for differentiability.\n",
        "    \"\"\"\n",
        "    J = nb_spg * ny\n",
        "    z_val = z_grid_t[iz]\n",
        "    r_val = r_grid_t[ir]\n",
        "    c = theta_to_consumption_grid(theta, b_grid_t, y_grid_t, z_grid_t, r_grid_t)\n",
        "    c_val = c[:, iz, ir]  # (J,)\n",
        "    b_next = (1 + r_val) * b_grid_t.repeat_interleave(ny) + y_grid_t.repeat(nb_spg) * z_val - c_val\n",
        "    b_next = torch.clamp(b_next, b_min, b_max)\n",
        "    # Soft weights: weight_ib = exp(-(b'-b_grid[ib])^2/(2*sigma^2)), normalized\n",
        "    dist = b_next.unsqueeze(1) - b_grid_t.unsqueeze(0)  # (J, nb)\n",
        "    w_b = torch.exp(-dist.pow(2) / (2 * sigma_b**2))\n",
        "    w_b = w_b / (w_b.sum(dim=1, keepdim=True) + 1e-8)\n",
        "    # A_π: from j=(ib,iy) to j'=(ib',iy'): weight_b(ib') * Ty[iy, iy']\n",
        "    A = torch.zeros(J, J, dtype=dtype, device=device)\n",
        "    for j in range(J):\n",
        "        ib, iy = j // ny, j % ny\n",
        "        for ibp in range(nb_spg):\n",
        "            for iyp in range(ny):\n",
        "                jp = ibp * ny + iyp\n",
        "                A[jp, j] = w_b[j, ibp] * Ty_t[iy, iyp]\n",
        "    return A\n",
        "\n",
        "def aggregate_saving_grid(theta, G, iz, ir, b_grid_t, y_grid_t, z_grid_t, r_grid_t, ny):\n",
        "    \"\"\"Aggregate saving G^T b'(θ,z,r); G = fraction of agents in each state. Market clearing.\"\"\"\n",
        "    J = G.shape[0]\n",
        "    z_val = z_grid_t[iz]\n",
        "    r_val = r_grid_t[ir]\n",
        "    c = theta_to_consumption_grid(theta, b_grid_t, y_grid_t, z_grid_t, r_grid_t)[:, iz, ir]\n",
        "    b_next = (1 + r_val) * b_grid_t.repeat_interleave(ny) + y_grid_t.repeat(nb_spg) * z_val - c\n",
        "    b_next = torch.clamp(b_next, b_min, b_max)\n",
        "    return (G * b_next).sum()\n",
        "\n",
        "def P_star_detach(theta, G, iz, b_grid_t, y_grid_t, z_grid_t, r_grid_t, ny, B=0.0, beta_t=0.96, sigma_t=2.0):\n",
        "    \"\"\"Find r s.t. aggregate_saving ≈ B. S(r) is monotonic in r, so use bisection. Initial guess: r ≈ 1/β - 1 (Euler).\"\"\"\n",
        "    nr = len(r_grid_t)\n",
        "    if nr == 1:\n",
        "        return r_grid_t[0].detach()\n",
        "    # Initial guess: steady-state type r_guess = 1/β - 1; c = ((1+r) - β*(1+r)^(1/σ))*(b + yz/r) for MPC\n",
        "    r_guess = (1.0 / beta_t) - 1.0\n",
        "    r_guess = max(min(r_guess, r_grid_t[-1].item()), r_grid_t[0].item())\n",
        "    ir_mid = int(torch.searchsorted(r_grid_t, torch.tensor(r_guess, device=r_grid_t.device, dtype=r_grid_t.dtype)).clamp(0, nr - 1).item())\n",
        "    def S_at(ir):\n",
        "        return aggregate_saving_grid(theta, G, iz, ir, b_grid_t, y_grid_t, z_grid_t, r_grid_t, ny).item()\n",
        "    # Bisection: assume S(ir) increasing in ir; find [ir_lo, ir_hi] that bracket B\n",
        "    s_lo, s_hi = S_at(0), S_at(nr - 1)\n",
        "    if s_lo >= B:\n",
        "        best_ir = 0\n",
        "    elif s_hi <= B:\n",
        "        best_ir = nr - 1\n",
        "    else:\n",
        "        ir_lo, ir_hi = 0, nr - 1\n",
        "        s_guess = S_at(ir_mid)\n",
        "        if s_guess < B:\n",
        "            ir_lo = ir_mid\n",
        "        else:\n",
        "            ir_hi = ir_mid\n",
        "        while ir_hi - ir_lo > 1:\n",
        "            ir_mid = (ir_lo + ir_hi) // 2\n",
        "            s_mid = S_at(ir_mid)\n",
        "            if s_mid < B:\n",
        "                ir_lo = ir_mid\n",
        "            else:\n",
        "                ir_hi = ir_mid\n",
        "        best_ir = ir_lo if abs(S_at(ir_lo) - B) <= abs(S_at(ir_hi) - B) else ir_hi\n",
        "    return r_grid_t[best_ir].detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34969366",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "39fb92c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Map scalar r (e.g. detached) to grid index for policy lookup\n",
        "def r_to_ir(r_val, r_grid_t):\n",
        "    r_np = r_grid_t.detach().cpu().numpy()\n",
        "    return int(np.clip(np.searchsorted(r_np, r_val.item() if torch.is_tensor(r_val) else r_val), 0, len(r_np)-1))\n",
        "\n",
        "# Utility on grid (vector over J)\n",
        "def u_torch(c_vec, sig=sigma):\n",
        "    c_vec = torch.clamp(c_vec, min=c_min)\n",
        "    if abs(sig - 1.0) < 1e-8:\n",
        "        return torch.log(c_vec)\n",
        "    return (c_vec ** (1 - sig)) / (1 - sig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "18548579",
      "metadata": {},
      "outputs": [],
      "source": [
        "# G = cross-sectional distribution (fraction of states). G evolves via transition matrix A_π from policy π.\n",
        "# L(θ) = (1/N) Σ_n Σ_t β^t G_t^T u(c_t);  G_{t+1} = A_π @ G_t  (not simple addition).\n",
        "def spg_objective(theta, N_traj, T_horizon, b_grid_t, y_grid_t, z_grid_t, r_grid_t, Ty_t, Tz_t,\n",
        "                  nb_spg, ny, nz_spg, nr_spg, beta_t, G0=None):\n",
        "    \"\"\"\n",
        "    G_t = fraction of agents in each state j. Transition A_π from π: A_π[j',j]=P(s'=j'|s=j). G_{t+1}=A_π @ G_t.\n",
        "    Price r_t = P*(G_t, z_t) with gradient-stop.\n",
        "    \"\"\"\n",
        "    J = nb_spg * ny\n",
        "    if G0 is None:\n",
        "        G0 = torch.ones(J, device=device, dtype=dtype) / J\n",
        "    L_list = []\n",
        "    for n in range(N_traj):\n",
        "        iz = np.random.randint(0, nz_spg)\n",
        "        G = G0.clone()\n",
        "        L_n = torch.tensor(0.0, device=device, dtype=dtype)\n",
        "        for t in range(T_horizon):\n",
        "            r_t = P_star_detach(theta, G, iz, b_grid_t, y_grid_t, z_grid_t, r_grid_t, ny)\n",
        "            ir = r_to_ir(r_t, r_grid_t)\n",
        "            c = theta_to_consumption_grid(theta, b_grid_t, y_grid_t, z_grid_t, r_grid_t)\n",
        "            c_t = c[:, iz, ir]\n",
        "            L_n = L_n + (beta_t ** t) * (G @ u_torch(c_t))\n",
        "            A_pi = build_A_pi(theta, iz, ir, b_grid_t, y_grid_t, z_grid_t, r_grid_t, Ty_t, nb_spg, ny)\n",
        "            G = A_pi @ G\n",
        "            cum = np.cumsum(Tz_t[iz, :].detach().cpu().numpy())\n",
        "            iz = min(np.searchsorted(cum, np.random.rand()), nz_spg - 1)\n",
        "        L_list.append(L_n)\n",
        "    return torch.stack(L_list).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c021831",
      "metadata": {},
      "source": [
        "## Using Trained Policy\n",
        "\n",
        "After training, convert theta to consumption grid and use `policy_from_grid` to get consumption strategy π(b, y, r, z) from the trained grid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a358a0ca",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'theta' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# After training, convert theta to consumption grid\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m theta_trained = \u001b[43mtheta\u001b[49m.detach()  \u001b[38;5;66;03m# Use trained theta (no gradient needed for simulation)\u001b[39;00m\n\u001b[32m      3\u001b[39m c_grid_trained = theta_to_consumption_grid(theta_trained, b_grid_spg, y_grid_t, z_grid_t, r_grid_t)\n\u001b[32m      4\u001b[39m c_grid_np = c_grid_trained.cpu().numpy()  \u001b[38;5;66;03m# Convert to numpy for policy_from_grid\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'theta' is not defined"
          ]
        }
      ],
      "source": [
        "# After training, convert theta to consumption grid\n",
        "theta_trained = theta.detach()  # Use trained theta (no gradient needed for simulation)\n",
        "c_grid_trained = theta_to_consumption_grid(theta_trained, b_grid_spg, y_grid_t, z_grid_t, r_grid_t)\n",
        "c_grid_np = c_grid_trained.cpu().numpy()  # Convert to numpy for policy_from_grid\n",
        "\n",
        "# Create policy function using trained grid\n",
        "def policy_trained(b, y, r, z):\n",
        "    \"\"\"Policy function using trained theta grid\"\"\"\n",
        "    return policy_from_grid(\n",
        "        b, y, r, z, \n",
        "        c_grid_np,  # Consumption grid from trained theta\n",
        "        b_grid_spg.cpu().numpy(),\n",
        "        y_grid_t.cpu().numpy(),\n",
        "        z_grid_t.cpu().numpy(),\n",
        "        r_grid_t.cpu().numpy(),\n",
        "        ny\n",
        "    )\n",
        "\n",
        "# Example: test the trained policy\n",
        "print(\"Testing trained policy:\")\n",
        "b_test = np.array([0.5, 1.0, 2.0])\n",
        "y_test = y_grid[1]  # Use middle income level\n",
        "r_test = (r_min + r_max) / 2.0  # mid-grid r for testing\n",
        "z_test = z_grid[len(z_grid)//2]  # Use middle z value\n",
        "\n",
        "c_test, b_next_test = policy_trained(b_test, y_test, r_test, z_test)\n",
        "print(f\"  b = {b_test}, y = {y_test:.3f}, r = {r_test:.3f}, z = {z_test:.3f}\")\n",
        "print(f\"  c = {c_test}, b_next = {b_next_test}\")\n",
        "print(f\"  Budget check: (1+r)*b + y*z = {(1+r_test)*b_test + y_test*z_test}\")\n",
        "print(f\"  Budget satisfied: c + b_next = {c_test + b_next_test}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8c3c450",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize θ and run SPG (gradient ascent on L(θ))\n",
        "nr_spg = len(r_grid_t)\n",
        "nz_spg = len(z_grid_t)\n",
        "theta = init_theta(b_grid_spg, y_grid_t, z_grid_t, r_grid_t)\n",
        "theta = theta.requires_grad_(True)\n",
        "optimizer = torch.optim.Adam([theta], lr=1e-3)\n",
        "N_traj = 32\n",
        "T_horizon = 30\n",
        "n_epochs = 100\n",
        "loss_hist = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    L = spg_objective(theta, N_traj, T_horizon, b_grid_spg, y_grid_t, z_grid_t, r_grid_t, Ty_t, Tz_t,\n",
        "                      nb_spg, ny, nz_spg, nr_spg, beta)\n",
        "    loss_hist.append(L.item())\n",
        "    (-L).backward()\n",
        "    optimizer.step()\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f\"Epoch {epoch+1}, L(θ) = {L.item():.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3a01f5e",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "- **Model:** Huggett (1993) with aggregate risk: bond market clearing $r_t = P^*(G_t, z_t)$, policy $\\pi(b,y,r,z) \\to (c, b')$.\n",
        "- **State dynamics:** $y$, $z$ follow transition matrices $T_y$, $T_z$ (Tauchen); draws use `np.random.choice(..., p=row)`.\n",
        "- **SRL/SPG:** Maximize $L(\\theta) = \\mathbb{E}[\\sum_t \\beta^t d_t^\\top u(c_t)]$ with stop-gradient on $r$; distribution $G$ updated via $A_\\pi(r,z)$ (Young lottery). Trained policy can be used in `policy_from_grid` for simulation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
