{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Krusell–Smith (1998) Model with SRL\n",
        "\n",
        "Heterogeneous-agent economy: **wealth is productive capital** rented to a representative firm. Production $Y_t = z_t K_t^\\alpha L_t^{1-\\alpha}$ with **total labour $L_t = 1$** (fixed). Factor prices: $r_t = \\alpha z_t K_t^{\\alpha-1} - \\delta$, $w_t = (1-\\alpha) z_t K_t^\\alpha$.\n",
        "\n",
        "**Policy (consumption, labour):** $(c, n) = \\pi(b, y; r, w)$ — consumption and labour supply depend on **prices** $(r, w)$.  \n",
        "**Individual state:** $(b, y)$ — capital and idiosyncratic labour productivity.  \n",
        "**Aggregate:** $K_t = \\int b\\, dG_t$; $(r_t, w_t) = P^*(K_t, z_t)$ with $L=1$.  \n",
        "**Budget:** $c + b' = (1+r)b + w\\, n\\, y$, $b \\geq 0$, $n \\in [0,1]$. We assume **total labour supply** $\\int n\\, dG = 1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3c4a686",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== 依赖库 / Dependencies ==========\n",
        "# 中文: numpy 数值计算与网格；matplotlib 作图；scipy.stats.norm 为 Tauchen 离散化提供正态 CDF\n",
        "# EN: numpy for arrays/grids; matplotlib for plots; scipy.stats.norm for Tauchen (normal CDF)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "np.random.seed(42)   # 固定随机种子，便于复现 / fix seed for reproducibility\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['axes.unicode_minus'] = False  # 避免图中负号显示为方块 / avoid minus sign rendering as block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calibration (SRL Section 4.2 & Appendix A.2, Table 4 & 5)\n",
        "\n",
        "- **Preferences:** $\\beta=0.95$, CRRA $\\sigma=3$.\n",
        "- **Production:** capital share $\\alpha=0.36$, depreciation $\\delta=0.08$.\n",
        "- **Idiosyncratic income** $y$: same as Huggett (log AR(1), $\\rho_y=0.6$, $\\nu_y=0.2$), 3 states.\n",
        "- **Aggregate TFP** $z$: log AR(1), $\\rho_z=0.9$, $\\nu_z=0.03$.\n",
        "- **Capital:** $b\\geq 0$, grid $n_b=200$, $b_{\\max}=100$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4523a67e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== 校准参数 / Calibration (SRL Section 4.2 & Appendix A.2, Table 4 & 5) ==========\n",
        "\n",
        "# --- 偏好 / Preferences ---\n",
        "beta = 0.95       # 折现因子 / discount factor\n",
        "sigma = 3.0       # CRRA 系数 σ / CRRA coefficient\n",
        "\n",
        "# --- 生产 / Production ---\n",
        "alpha = 0.36\n",
        "delta = 0.08\n",
        "\n",
        "# 个体收入 y（与 Huggett 一致：水平 AR(1)，3 状态）/ Idiosyncratic y: same as Huggett, 3 states\n",
        "rho_y = 0.6\n",
        "nu_y = 0.2\n",
        "ny = 3\n",
        "\n",
        "# 总量 z：log z ~ AR(1) / Aggregate TFP z: log z ~ AR(1)\n",
        "rho_z = 0.9\n",
        "nu_z = 0.03\n",
        "\n",
        "# 资本网格 / Capital grid\n",
        "b_min = 0.0\n",
        "b_max = 100.0\n",
        "nb = 200\n",
        "J = nb * ny       # 复合状态数 j = ib*ny+iy / composite state size\n",
        "\n",
        "# 价格网格（政策查表）/ Price grids for policy (Table 5)\n",
        "nr = 30\n",
        "r_min, r_max = 0.02, 0.07\n",
        "nw = 50\n",
        "w_min, w_max = 0.9, 1.5\n",
        "nz = 30\n",
        "\n",
        "# 数值截断 / Truncation (same style as Huggett)\n",
        "c_min = 1e-3\n",
        "T_trunc = 90\n",
        "etrunc = 1e-2\n",
        "\n",
        "# ========== SPG training (same style as Huggett Table 3) ==========\n",
        "N_epoch = 200\n",
        "N_warmup = 25\n",
        "lr_ini = 5e-4\n",
        "lr_decay = 0.5\n",
        "N_sample = 32      # 每轮轨迹数 / trajectories per update\n",
        "e_converge = 3e-4\n",
        "\n",
        "print(\"Krusell-Smith calibration (SRL 4.2, App A.2):\")\n",
        "print(f\"  β={beta}, σ={sigma}, α={alpha}, δ={delta}\")\n",
        "print(f\"  ρy={rho_y}, νy={nu_y}, ρz={rho_z}, νz={nu_z}\")\n",
        "print(f\"  b∈[0,{b_max}], nb={nb}, ny={ny}, J={J}; r∈[{r_min},{r_max}], nr={nr}; w∈[{w_min},{w_max}], nw={nw}\")\n",
        "print(f\"  T_trunc={T_trunc}, c_min={c_min}\")\n",
        "print(\"  SPG: Nepoch=%d, Nwarmup=%d, lr_ini=%s, lr_decay=%s, Nsample=%d, econverge=%s\" % (N_epoch, N_warmup, lr_ini, lr_decay, N_sample, e_converge))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff0b3c44",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== Tauchen (1986) 离散化 AR(1)，与 Huggett 统一形式 / Discretize AR(1) (unified) ==========\n",
        "def tauchen_ar1(rho, sigma_innov, n_states, m=3, mean=0.0):\n",
        "    \"\"\"Unified Tauchen. mean=0 → zero-mean (e.g. log z); mean=μ → level process (e.g. y with E[y]=1).\"\"\"\n",
        "    std = sigma_innov / np.sqrt(1 - rho**2)\n",
        "    if mean == 0:\n",
        "        x_min, x_max = -m * std, m * std\n",
        "    else:\n",
        "        x_min = max(1e-6, mean - m * std)\n",
        "        x_max = mean + m * std\n",
        "    x_grid = np.linspace(x_min, x_max, n_states)\n",
        "    step = (x_max - x_min) / (n_states - 1) if n_states > 1 else 1.0\n",
        "    mu_i = (1 - rho) * mean + rho * x_grid\n",
        "    z_lo = (x_grid - mu_i[:, None] + step / 2) / sigma_innov\n",
        "    z_hi = (x_grid - mu_i[:, None] - step / 2) / sigma_innov\n",
        "    P = np.zeros((n_states, n_states))\n",
        "    P[:, 0] = norm.cdf(z_lo[:, 0])\n",
        "    P[:, -1] = 1 - norm.cdf(z_hi[:, -1])\n",
        "    if n_states > 2:\n",
        "        P[:, 1:-1] = norm.cdf(z_lo[:, 1:-1]) - norm.cdf(z_hi[:, 1:-1])\n",
        "    P = P / P.sum(axis=1, keepdims=True)\n",
        "    return x_grid, P\n",
        "\n",
        "# 个体收入 y：水平 AR(1)，mean=1 再归一化（与 Huggett 一致）/ Idiosyncratic y: level AR(1), E[y]=1\n",
        "y_grid, Ty = tauchen_ar1(rho_y, nu_y, ny, m=3, mean=1.0)\n",
        "invariant_y = np.linalg.matrix_power(Ty.T, 200)[:, 0]\n",
        "y_grid = y_grid / (y_grid @ invariant_y)\n",
        "\n",
        "# 总量 z：log z ~ AR(1) / Aggregate z: log z ~ AR(1)\n",
        "log_z_grid, Tz = tauchen_ar1(rho_z, nu_z, nz, mean=0.0)\n",
        "z_grid = np.exp(log_z_grid)\n",
        "invariant_z = np.linalg.matrix_power(Tz.T, 200)[:, 0]\n",
        "z_grid = z_grid / (z_grid @ invariant_z)\n",
        "\n",
        "b_grid = np.linspace(b_min, b_max, nb)\n",
        "r_grid = np.linspace(r_min, r_max, nr)\n",
        "w_grid = np.linspace(w_min, w_max, nw)\n",
        "\n",
        "# ---------- 效用函数（CRRA）/ Utility (CRRA; log(c) when σ=1) ----------\n",
        "def u(c, sig=sigma):\n",
        "    c = np.maximum(c, c_min)\n",
        "    if np.abs(sig - 1.0) < 1e-10:\n",
        "        return np.log(c)\n",
        "    return (c ** (1 - sig)) / (1 - sig)\n",
        "\n",
        "def u_prime(c, sig=sigma):\n",
        "    return np.maximum(c, c_min) ** (-sig)\n",
        "\n",
        "print(\"y_grid (idiosyncratic):\", y_grid)\n",
        "print(\"z_grid (first/last):\", z_grid[0], z_grid[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Production and prices\n",
        "\n",
        "Given aggregate capital $K$ and TFP $z$: $r^K = \\alpha z K^{\\alpha-1}$, $w = (1-\\alpha)z K^\\alpha$ (with $L=1$). Net return $r = r^K - \\delta$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0375efe8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def K_to_prices(K, z):\n",
        "    \"\"\"(K, z) -> (r_net, w). Total labour L=1 (fixed).\"\"\"\n",
        "    K = np.maximum(K, 1e-8)\n",
        "    rK = alpha * z * (K ** (alpha - 1))\n",
        "    w = (1 - alpha) * z * (K ** alpha)\n",
        "    r_net = rK - delta\n",
        "    return r_net, w\n",
        "\n",
        "# Example: steady-state K from β: 1 = β(1 + r) => r_ss = 1/beta - 1\n",
        "r_ss = 1.0 / beta - 1\n",
        "z_ss = 1.0\n",
        "K_ss = (alpha * z_ss / (r_ss + delta)) ** (1 / (1 - alpha))\n",
        "r_ss_check, w_ss_check = K_to_prices(K_ss, z_ss)\n",
        "print(f\"Steady state (z=1): r_ss={r_ss:.4f}, K_ss={K_ss:.4f}, w_ss={w_ss_check:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24076e08",
      "metadata": {},
      "source": [
        "## Policy and simulation (SRL: price-based)\n",
        "\n",
        "**Policy (consumption, labour):** $(c, n) = \\pi(b, y, r, w)$ from **prices** $(r, w)$. In GE, $(r_t, w_t) = P^*(K_t, z_t)$ with $K_t = \\int b\\, dG_t$ and **total labour** $L=1$. We **stop-gradient** on $(r,w)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19417d59",
      "metadata": {},
      "outputs": [],
      "source": [
        "def draw_next_state(y_idx, z_idx, Ty, Tz):\n",
        "    \"\"\"按 Ty、Tz 抽取下一期 (y',z') 的网格索引 / draw next (y,z) indices (same as Huggett).\"\"\"\n",
        "    y_next = np.random.choice(Ty.shape[1], p=Ty[y_idx, :])\n",
        "    z_next = np.random.choice(Tz.shape[1], p=Tz[z_idx, :])\n",
        "    return y_next, z_next\n",
        "\n",
        "def policy_placeholder(b, y, r, w, z, save_frac=0.25, n_labour=1.0):\n",
        "    \"\"\"(c, n, b') = π(b, y, r, w). Budget: c + b' = (1+r)*b + w*n*y. Returns raw (c, n, b'); n=1.\"\"\"\n",
        "    cash = (1 + r) * b + w * n_labour * y\n",
        "    b_next = np.clip(save_frac * cash, b_min, b_max)\n",
        "    c = np.maximum(cash - b_next, c_min)\n",
        "    n = np.full_like(b, n_labour) if np.isscalar(b) else np.full(len(b), n_labour)\n",
        "    return c, n, b_next\n",
        "\n",
        "# ========== 从网格 θ 得到政策 π(b,y,r,w,z)=(c,n,b')，只返回连续值；彩票在转移时用 / Policy from grid θ, raw (c,n,b') ==========\n",
        "def policy_from_grid_ks(b, y, r, w, z, theta_grid, b_grid, y_grid, z_grid, r_grid, w_grid, ny, c_min_val=1e-3, n_labour=1.0):\n",
        "    \"\"\"Input: (b,y), (r,w,z), theta_grid (c grid shape J,nz,nr,nw). Output: (c, n, b_next) raw; no lottery here.\"\"\"\n",
        "    if hasattr(theta_grid, 'detach'):\n",
        "        theta_grid = theta_grid.detach().cpu().numpy()\n",
        "    if hasattr(b_grid, 'detach'):\n",
        "        b_grid = b_grid.detach().cpu().numpy()\n",
        "    if hasattr(y_grid, 'detach'):\n",
        "        y_grid = y_grid.detach().cpu().numpy()\n",
        "    if hasattr(z_grid, 'detach'):\n",
        "        z_grid = z_grid.detach().cpu().numpy()\n",
        "    if hasattr(r_grid, 'detach'):\n",
        "        r_grid = r_grid.detach().cpu().numpy()\n",
        "    if hasattr(w_grid, 'detach'):\n",
        "        w_grid = w_grid.detach().cpu().numpy()\n",
        "    b_grid = np.asarray(b_grid)\n",
        "    y_grid = np.asarray(y_grid)\n",
        "    z_grid = np.asarray(z_grid)\n",
        "    r_grid = np.asarray(r_grid)\n",
        "    w_grid = np.asarray(w_grid)\n",
        "    ib = np.atleast_1d(np.clip(np.searchsorted(b_grid, b, side='right') - 1, 0, len(b_grid)-1))\n",
        "    iy = np.atleast_1d(np.clip(np.searchsorted(y_grid, y, side='right') - 1, 0, len(y_grid)-1))\n",
        "    iz = np.atleast_1d(np.clip(np.searchsorted(z_grid, z, side='right') - 1, 0, len(z_grid)-1))\n",
        "    ir = np.atleast_1d(np.clip(np.searchsorted(r_grid, r, side='right') - 1, 0, len(r_grid)-1))\n",
        "    iw = np.atleast_1d(np.clip(np.searchsorted(w_grid, w, side='right') - 1, 0, len(w_grid)-1))\n",
        "    b, y, r, w, z = np.atleast_1d(b, dtype=float), np.atleast_1d(y, dtype=float), np.atleast_1d(r, dtype=float), np.atleast_1d(w, dtype=float), np.atleast_1d(z, dtype=float)\n",
        "    j = ib * ny + iy\n",
        "    c = np.maximum(theta_grid[j, iz, ir, iw], c_min_val)\n",
        "    cash = (1 + r) * b + w * n_labour * y\n",
        "    b_next = np.clip(cash - c, b_min, b_max)\n",
        "    c = np.maximum(cash - b_next, c_min_val)\n",
        "    n = np.full_like(b, n_labour)\n",
        "    if c.size == 1:\n",
        "        return c.ravel()[0], n.ravel()[0], b_next.ravel()[0]\n",
        "    return c, n, b_next\n",
        "\n",
        "def b_next_to_grid_lottery(b_next_feasible, b_grid):\n",
        "    \"\"\"连续 b' 按 Young 法随机舍入到 b_grid 相邻两点（仅当需要离散 b' 时用）。\"\"\"\n",
        "    b_next_feasible = np.asarray(b_next_feasible)\n",
        "    i_lo = np.clip(np.searchsorted(b_grid, b_next_feasible, side='right') - 1, 0, len(b_grid) - 2)\n",
        "    gap = b_grid[i_lo + 1] - b_grid[i_lo]\n",
        "    lam = np.clip((b_next_feasible - b_grid[i_lo]) / (gap + 1e-12), 0.0, 1.0)\n",
        "    lottery = np.random.rand(len(np.atleast_1d(b_next_feasible))) < lam\n",
        "    return np.where(lottery, b_grid[i_lo + 1], b_grid[i_lo])\n",
        "\n",
        "def aggregate_capital(b_vec):\n",
        "    return np.mean(b_vec)\n",
        "\n",
        "def simulate_krusell_smith(T, N_agents, policy_fn, y_grid, z_grid, Ty, Tz, b0=None, z0_idx=None):\n",
        "    \"\"\"Simulate K-S: K_t=mean(b_t), (r_t,w_t)=P*(K_t,z_t) with L=1, (c,n,b')=π(b,y,r,w). y,z from Ty,Tz.\"\"\"\n",
        "    if z0_idx is None:\n",
        "        z0_idx = np.random.randint(0, len(z_grid))\n",
        "    y0_idx = np.random.randint(0, len(y_grid), size=N_agents)\n",
        "    if b0 is None:\n",
        "        b0 = np.full(N_agents, K_ss / N_agents)\n",
        "    b = b0.copy()\n",
        "    y_idx = y0_idx.copy()\n",
        "    z_idx = z0_idx\n",
        "    paths = {'b': np.zeros((T+1, N_agents)), 'y': np.zeros((T+1, N_agents)), 'z': np.zeros(T+1),\n",
        "             'r': np.zeros(T+1), 'w': np.zeros(T+1), 'K': np.zeros(T+1), 'c': np.zeros((T+1, N_agents)), 'n': np.zeros((T+1, N_agents))}\n",
        "    paths['b'][0], paths['y'][0], paths['z'][0] = b, y_grid[y_idx], z_grid[z_idx]\n",
        "    paths['K'][0] = np.mean(b)\n",
        "    paths['r'][0], paths['w'][0] = K_to_prices(paths['K'][0], paths['z'][0])\n",
        "    paths['c'][0], paths['n'][0] = np.nan, np.nan\n",
        "    for t in range(T):\n",
        "        K_t = np.mean(b)\n",
        "        z_t = z_grid[z_idx]\n",
        "        r_t, w_t = K_to_prices(K_t, z_t)\n",
        "        y_val = y_grid[y_idx]\n",
        "        c_t, n_t, b_next = policy_fn(b, y_val, r_t, w_t, z_t)\n",
        "        paths['r'][t], paths['w'][t], paths['K'][t] = r_t, w_t, K_t\n",
        "        paths['c'][t], paths['n'][t] = c_t, n_t\n",
        "        b = b_next\n",
        "        paths['b'][t+1] = b\n",
        "        _, z_idx = draw_next_state(y_idx[0], z_idx, Ty, Tz)\n",
        "        y_idx = np.array([draw_next_state(y_idx[i], z_idx, Ty, Tz)[0] for i in range(N_agents)])\n",
        "        paths['y'][t+1] = y_grid[y_idx]\n",
        "        paths['z'][t+1] = z_grid[z_idx]\n",
        "    paths['K'][T] = np.mean(b)\n",
        "    paths['r'][T], paths['w'][T] = K_to_prices(paths['K'][T], paths['z'][T])\n",
        "    c_T, n_T, _ = policy_placeholder(b, paths['y'][T], paths['r'][T], paths['w'][T], paths['z'][T])\n",
        "    paths['c'][T], paths['n'][T] = c_T, n_T\n",
        "    v_hat = np.sum([(beta**t) * np.mean(u(paths['c'][t])) for t in range(T)])\n",
        "    return paths, v_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2df09898",
      "metadata": {},
      "outputs": [],
      "source": [
        "policy_fn = lambda b, y, r, w, z: policy_placeholder(b, y, r, w, z, save_frac=0.25)\n",
        "T_sim, N_agents = 200, 2000\n",
        "paths, v_hat = simulate_krusell_smith(T_sim, N_agents, policy_fn, y_grid, z_grid, Ty, Tz)\n",
        "print(\"Simulation done. v̂_π =\", v_hat)\n",
        "print(\"Mean K:\", np.mean(paths['K']))\n",
        "print(\"Mean r, w:\", np.mean(paths['r']), np.mean(paths['w']))\n",
        "print(\"Mean n (total labour ≈ 1):\", np.nanmean(paths['n']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1106b8d",
      "metadata": {},
      "source": [
        "### Part A：给定政策下的分布模拟（与 Huggett 一致）/ Distribution simulation with given policy\n",
        "\n",
        "d 为分布 (nb,ny) 或 (J,)；K = d·b（当期资本）；(r,w)=P*(K,z)；update_d_direct 用 Young 法 d_{t+1} 不建 A。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64d079dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def d_to_mat(d):\n",
        "    \"\"\"统一为 (nb, ny) 矩阵；若已是 (J,) 则 reshape。\"\"\"\n",
        "    d = np.asarray(d)\n",
        "    if d.ndim == 1:\n",
        "        return d.reshape(nb, ny)\n",
        "    return d\n",
        "\n",
        "def d_to_flat(d):\n",
        "    \"\"\"转为 (J,) 向量，j = ib*ny+iy。\"\"\"\n",
        "    return np.asarray(d).reshape(nb, ny).ravel()\n",
        "\n",
        "# 总资本 K = d·b（当期资产加权），用于 (r,w)=K_to_prices(K,z)\n",
        "def aggregate_capital_from_d(d, b_grid, y_grid, ny):\n",
        "    \"\"\"K = Σ_j d(j)*b(j) = d·b_vals。\"\"\"\n",
        "    nb_ = len(b_grid)\n",
        "    b_vals = np.repeat(b_grid, ny)\n",
        "    return np.dot(d_to_flat(d), b_vals)\n",
        "\n",
        "# d_t → d_{t+1} 直接映射（Young 法），不建 A；policy 返回 (c, n, b_next) 连续\n",
        "def update_d_direct(d, r, w, z, policy_fn, b_grid, y_grid, Ty):\n",
        "    \"\"\"从 d 和 policy 直接得到 d_new；Young 权重散射后 Q @ Ty。\"\"\"\n",
        "    nb_, ny_ = len(b_grid), len(y_grid)\n",
        "    J_ = nb_ * ny_\n",
        "    d_flat = d_to_flat(d)\n",
        "    b_flat = np.repeat(b_grid, ny_)\n",
        "    y_flat = np.tile(y_grid, nb_)\n",
        "    c, n, b_next_feasible = policy_fn(b_flat, y_flat, r, w, z)\n",
        "    i_lo = np.clip(np.searchsorted(b_grid, b_next_feasible, side='right') - 1, 0, nb_ - 2)\n",
        "    gap = b_grid[i_lo + 1] - b_grid[i_lo]\n",
        "    lam = np.clip((b_next_feasible - b_grid[i_lo]) / (gap + 1e-12), 0.0, 1.0)\n",
        "    iy_all = np.arange(J_) % ny_\n",
        "    Q = np.zeros((nb_, ny_))\n",
        "    np.add.at(Q, (i_lo, iy_all), (1 - lam) * d_flat)\n",
        "    np.add.at(Q, (i_lo + 1, iy_all), lam * d_flat)\n",
        "    d_new = Q @ Ty\n",
        "    d_new = d_new / (d_new.sum() + 1e-20)\n",
        "    return d_new\n",
        "\n",
        "# 可选：分布层面模拟（每期 K_t=d·b, (r,w)=P*(K,z), d_{t+1}=update_d_direct）\n",
        "# paths_d = simulate_krusell_smith_distribution(T, policy_fn, b_grid, y_grid, z_grid, Ty, Tz, d0=None, z0_idx=None)\n",
        "# 实现方式同 Huggett 的 simulate_huggett，此处略。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0cabe4a",
      "metadata": {},
      "source": [
        "## SRL / SPG: Grid policy $(c,n)(b,y,r,w)$ with gradient-stop on $(r,w)$\n",
        "\n",
        "Policy $(c, n) = \\pi(b, y; r, w)$; total labour $L=1$. Same as Huggett: maximize $L(\\theta) = \\mathbb{E}[\\sum_t \\beta^t d_t^\\top u(c_t)]$; $K_t$ from $d_t$, $(r_t,w_t)=P^*(K_t,z_t)$ **detached**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aca021a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== Part B：SRL/SPG 训练（θ 可微；d 用向量 (J,)；软权重、P* detach）/ Part B: SRL training ==========\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "dtype = torch.float32\n",
        "\n",
        "# SPG 用较粗网格加速（Table 5: nb=200, nr=30, nw=50；此处缩小以加速）\n",
        "nb_spg, nr_spg, nw_spg, nz_spg = 50, 15, 25, 10\n",
        "ny_spg = ny\n",
        "J = nb_spg * ny_spg\n",
        "\n",
        "b_grid_t = torch.tensor(np.linspace(b_min, b_max, nb_spg), dtype=dtype, device=device)\n",
        "r_grid_t = torch.tensor(np.linspace(r_min, r_max, nr_spg), dtype=dtype, device=device)\n",
        "w_grid_t = torch.tensor(np.linspace(w_min, w_max, nw_spg), dtype=dtype, device=device)\n",
        "z_grid_t = torch.tensor(z_grid[np.linspace(0, nz-1, nz_spg, dtype=int)], dtype=dtype, device=device)\n",
        "y_grid_t = torch.tensor(y_grid, dtype=dtype, device=device)\n",
        "Ty_t = torch.tensor(Ty, dtype=dtype, device=device)\n",
        "iz_spg = np.linspace(0, nz-1, nz_spg, dtype=int)\n",
        "Tz_sub = Tz[np.ix_(iz_spg, iz_spg)]\n",
        "Tz_sub = Tz_sub / Tz_sub.sum(axis=1, keepdims=True)\n",
        "Tz_t = torch.tensor(Tz_sub, dtype=dtype, device=device)\n",
        "nz_spg = Tz_t.shape[0]\n",
        "\n",
        "# 政策参数 θ → 消费网格 c = softplus(θ)+c_min（保证 c>0 且可导，同 Huggett）\n",
        "def theta_to_c_grid(theta, b_grid_t, y_grid_t, z_grid_t, r_grid_t, w_grid_t, c_min_val=None):\n",
        "    \"\"\"θ (J, nz, nr, nw) -> c >= c_min.\"\"\"\n",
        "    if c_min_val is None:\n",
        "        c_min_val = c_min\n",
        "    return torch.nn.functional.softplus(theta) + c_min_val\n",
        "\n",
        "# 初值：常数储蓄率规则得 c，反解 θ（同 Huggett init_theta）\n",
        "def init_theta_ks(b_grid_t, y_grid_t, z_grid_t, r_grid_t, w_grid_t, save_frac=0.25):\n",
        "    nz, nr, nw = len(z_grid_t), len(r_grid_t), len(w_grid_t)\n",
        "    c_grid = torch.zeros(J, nz, nr, nw, dtype=dtype, device=device)\n",
        "    for ib in range(nb_spg):\n",
        "        for iy in range(ny_spg):\n",
        "            j = ib * ny_spg + iy\n",
        "            b = b_grid_t[ib].item()\n",
        "            y = y_grid_t[iy].item()\n",
        "            for iz in range(nz):\n",
        "                z = z_grid_t[iz].item()\n",
        "                for ir in range(nr):\n",
        "                    r = r_grid_t[ir].item()\n",
        "                    for iw in range(nw):\n",
        "                        w = w_grid_t[iw].item()\n",
        "                        cash = (1 + r) * b + w * y\n",
        "                        c_grid[j, iz, ir, iw] = max((1 - save_frac) * cash, c_min)\n",
        "    x = c_grid - c_min\n",
        "    return torch.log(torch.exp(x) - 1 + 1e-8)\n",
        "\n",
        "def K_from_d(d, b_grid_t, ny_spg):\n",
        "    \"\"\"Aggregate capital K = d^T b (over b indices).\"\"\"\n",
        "    b_vals = b_grid_t.repeat_interleave(ny_spg)\n",
        "    return (d * b_vals).sum()\n",
        "\n",
        "def P_star_detach_ks(theta, d, iz, b_grid_t, y_grid_t, z_grid_t, r_grid_t, w_grid_t, ny_spg):\n",
        "    \"\"\"(K,z) -> (r,w) from production; K = d·b. Return (r,w) detached.\"\"\"\n",
        "    K = K_from_d(d, b_grid_t, ny_spg).item()\n",
        "    z_val = z_grid_t[iz].item()\n",
        "    r_net, w_val = K_to_prices(K, z_val)\n",
        "    r_t = torch.tensor(r_net, device=device, dtype=dtype)\n",
        "    w_t = torch.tensor(w_val, device=device, dtype=dtype)\n",
        "    return r_t.detach(), w_t.detach()\n",
        "\n",
        "def rw_to_ir_iw(r_val, w_val, r_grid_t, w_grid_t):\n",
        "    rn, wn = r_grid_t.cpu().numpy(), w_grid_t.cpu().numpy()\n",
        "    ir = int(np.clip(np.searchsorted(rn, r_val), 0, len(rn)-1))\n",
        "    iw = int(np.clip(np.searchsorted(wn, w_val), 0, len(wn)-1))\n",
        "    return ir, iw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22c04323",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== d_t -> d_{t+1} direct（向量化 Q@Ty，不建 J×J）/ same as Huggett update_G_pi_direct ==========\n",
        "def update_d_pi_direct_ks(theta, d, iz, ir, iw, b_grid_t, y_grid_t, z_grid_t, r_grid_t, w_grid_t, Ty_t, nb_spg, ny_spg, sigma_b=0.5):\n",
        "    \"\"\"d_new = (implied A) @ d；用 Q(ib',iy)=sum_ib w_b(ib*ny+iy,ib')*d(ib,iy)，再 d_new = Q @ Ty，无 J 循环、不建 A。\"\"\"\n",
        "    J = nb_spg * ny_spg\n",
        "    d_mat = d.view(nb_spg, ny_spg) if d.dim() == 1 else d\n",
        "    z_val = z_grid_t[iz]\n",
        "    r_val = r_grid_t[ir]\n",
        "    w_val = w_grid_t[iw]\n",
        "    c = theta_to_c_grid(theta, b_grid_t, y_grid_t, z_grid_t, r_grid_t, w_grid_t)[:, iz, ir, iw]\n",
        "    b_next = (1 + r_val) * b_grid_t.repeat_interleave(ny_spg) + y_grid_t.repeat(nb_spg) * w_val - c\n",
        "    b_next = torch.clamp(b_next, b_min, b_max)\n",
        "    dist = b_next.unsqueeze(1) - b_grid_t.unsqueeze(0)\n",
        "    w_b = torch.exp(-dist.pow(2) / (2 * sigma_b**2))\n",
        "    w_b = w_b / (w_b.sum(dim=1, keepdim=True) + 1e-8)\n",
        "    M = w_b.view(nb_spg, ny_spg, nb_spg).permute(2, 0, 1)\n",
        "    Q = (M * d_mat.unsqueeze(0)).sum(dim=1)\n",
        "    d_new = (Q @ Ty_t).reshape(-1)\n",
        "    return d_new / (d_new.sum() + 1e-20)\n",
        "\n",
        "def build_A_pi_ks(theta, iz, ir, iw, b_grid_t, y_grid_t, z_grid_t, r_grid_t, w_grid_t, Ty_t, nb_spg, ny_spg, sigma_b=0.5):\n",
        "    \"\"\"(Optional) Build full A_π; use update_d_pi_direct_ks in training. Evolution: d_new = A @ d.\"\"\"\n",
        "    J = nb_spg * ny_spg\n",
        "    z_val = z_grid_t[iz]\n",
        "    r_val = r_grid_t[ir]\n",
        "    w_val = w_grid_t[iw]\n",
        "    c = theta_to_c_grid(theta, b_grid_t, y_grid_t, z_grid_t, r_grid_t, w_grid_t)[:, iz, ir, iw]\n",
        "    b_next = (1 + r_val) * b_grid_t.repeat_interleave(ny_spg) + y_grid_t.repeat(nb_spg) * w_val - c\n",
        "    b_next = torch.clamp(b_next, b_min, b_max)\n",
        "    dist = b_next.unsqueeze(1) - b_grid_t.unsqueeze(0)\n",
        "    w_b = torch.exp(-dist.pow(2) / (2 * sigma_b**2))\n",
        "    w_b = w_b / (w_b.sum(dim=1, keepdim=True) + 1e-8)\n",
        "    A = torch.zeros(J, J, dtype=dtype, device=device)\n",
        "    for j in range(J):\n",
        "        ib, iy = j // ny_spg, j % ny_spg\n",
        "        for ibp in range(nb_spg):\n",
        "            for iyp in range(ny_spg):\n",
        "                jp = ibp * ny_spg + iyp\n",
        "                A[jp, j] = w_b[j, ibp] * Ty_t[iy, iyp]\n",
        "    return A\n",
        "\n",
        "def u_torch(c_vec, sig=sigma):\n",
        "    \"\"\"CRRA；σ=1 时为 log(c)，同 Huggett。\"\"\"\n",
        "    c_vec = torch.clamp(c_vec, min=c_min)\n",
        "    if abs(sig - 1.0) < 1e-8:\n",
        "        return torch.log(c_vec)\n",
        "    return (c_vec ** (1 - sig)) / (1 - sig)\n",
        "\n",
        "# Steady-state d0：用 update_d_pi_direct_ks 迭代（不建 A），与 Huggett steady_state_G0 一致\n",
        "def steady_state_d0_ks(theta, b_grid_t, y_grid_t, z_grid_t, r_grid_t, w_grid_t, Ty_t, nb_spg, ny_spg, nz_spg, nr_spg, nw_spg, n_iter=150):\n",
        "    J = nb_spg * ny_spg\n",
        "    iz_mid = nz_spg // 2\n",
        "    ir_mid = max(0, nr_spg // 2)\n",
        "    iw_mid = max(0, nw_spg // 2)\n",
        "    with torch.no_grad():\n",
        "        d = torch.ones(J, device=device, dtype=dtype) / J\n",
        "        for _ in range(n_iter):\n",
        "            d = update_d_pi_direct_ks(theta, d, iz_mid, ir_mid, iw_mid, b_grid_t, y_grid_t, z_grid_t, r_grid_t, w_grid_t, Ty_t, nb_spg, ny_spg)\n",
        "    return d\n",
        "\n",
        "def spg_objective_ks(theta, N_traj, T_horizon, b_grid_t, y_grid_t, z_grid_t, r_grid_t, w_grid_t, Ty_t, Tz_t,\n",
        "                       nb_spg, ny_spg, nz_spg, nr_spg, nw_spg, beta_t, d0=None, warm_up=False):\n",
        "    \"\"\"warm_up=True: d stays at d0 (no update). warm_up=False: d evolves via update_d_pi_direct_ks (d_new = A @ d).\"\"\"\n",
        "    J = nb_spg * ny_spg\n",
        "    if d0 is None:\n",
        "        d0 = torch.ones(J, device=device, dtype=dtype) / J\n",
        "    L_list = []\n",
        "    for n in range(N_traj):\n",
        "        iz = np.random.randint(0, nz_spg)\n",
        "        d = d0.clone()\n",
        "        L_n = torch.tensor(0.0, device=device, dtype=dtype)\n",
        "        for t in range(T_horizon):\n",
        "            r_t, w_t = P_star_detach_ks(theta, d, iz, b_grid_t, y_grid_t, z_grid_t, r_grid_t, w_grid_t, ny_spg)\n",
        "            ir, iw = rw_to_ir_iw(r_t.item(), w_t.item(), r_grid_t, w_grid_t)\n",
        "            c = theta_to_c_grid(theta, b_grid_t, y_grid_t, z_grid_t, r_grid_t, w_grid_t)\n",
        "            c_t = c[:, iz, ir, iw]\n",
        "            L_n = L_n + (beta_t ** t) * (d @ u_torch(c_t))\n",
        "            if not warm_up:\n",
        "                d = update_d_pi_direct_ks(theta, d, iz, ir, iw, b_grid_t, y_grid_t, z_grid_t, r_grid_t, w_grid_t, Ty_t, nb_spg, ny_spg)\n",
        "            iz = np.random.choice(nz_spg, p=Tz_t[iz, :].detach().cpu().numpy())\n",
        "        L_list.append(L_n)\n",
        "    return torch.stack(L_list).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1071a24",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== Init θ and run SPG：两阶段（warm-up 固定 d0，再 d 演化），与 Huggett 一致 ==========\n",
        "T_horizon = min(T_trunc, 50)   # 每条轨迹期数\n",
        "\n",
        "theta = init_theta_ks(b_grid_t, y_grid_t, z_grid_t, r_grid_t, w_grid_t)\n",
        "theta = theta.requires_grad_(True)\n",
        "optimizer = torch.optim.Adam([theta], lr=lr_ini)\n",
        "d0_steady = steady_state_d0_ks(theta, b_grid_t, y_grid_t, z_grid_t, r_grid_t, w_grid_t, Ty_t, nb_spg, ny_spg, nz_spg, nr_spg, nw_spg)\n",
        "\n",
        "loss_hist = []\n",
        "for epoch in range(N_epoch):\n",
        "    warm_up = (epoch < N_warmup)\n",
        "    t0 = max(epoch - N_warmup, 0) / max(N_epoch - N_warmup, 1)\n",
        "    lr_t = lr_ini * (lr_decay ** t0)\n",
        "    for g in optimizer.param_groups:\n",
        "        g['lr'] = lr_t\n",
        "    theta_old = theta.detach().clone()\n",
        "    optimizer.zero_grad()\n",
        "    d0_phase = d0_steady.detach() if warm_up else None\n",
        "    L = spg_objective_ks(theta, N_sample, T_horizon, b_grid_t, y_grid_t, z_grid_t, r_grid_t, w_grid_t,\n",
        "                          Ty_t, Tz_t, nb_spg, ny_spg, nz_spg, nr_spg, nw_spg, beta, d0=d0_phase, warm_up=warm_up)\n",
        "    loss_hist.append(L.item())\n",
        "    (-L).backward()\n",
        "    optimizer.step()\n",
        "    param_change = (theta.detach() - theta_old).abs().max().item()\n",
        "    if param_change < e_converge:\n",
        "        print(f\"Converged at epoch {epoch+1}, |Δθ| = {param_change:.2e}\")\n",
        "        break\n",
        "    if (epoch + 1) % 50 == 0 or (epoch + 1) <= 3 or (epoch + 1) == N_warmup:\n",
        "        phase = \"warm-up (d fixed)\" if warm_up else \"adaptive (d evolves)\"\n",
        "        print(f\"Epoch {epoch+1}, L(θ) = {L.item():.6f}, lr = {lr_t:.2e}, {phase}\")\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(loss_hist)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('L(θ)')\n",
        "plt.title('Krusell-Smith SRL/SPG (two-phase)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e541a0de",
      "metadata": {},
      "source": [
        "## Using Trained Policy\n",
        "\n",
        "After training, use `policy_from_grid_ks` with the consumption grid from θ (same idea as Huggett's policy_from_grid)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51b5c104",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- 使用训练好的政策：θ→消费网格→policy_from_grid_ks / Use trained policy ----------\n",
        "try:\n",
        "    theta_trained = theta.detach()\n",
        "    c_grid_ks = theta_to_c_grid(theta_trained, b_grid_t, y_grid_t, z_grid_t, r_grid_t, w_grid_t)\n",
        "    c_grid_ks_np = c_grid_ks.cpu().numpy()\n",
        "    def policy_trained_ks(b, y, r, w, z):\n",
        "        return policy_from_grid_ks(b, y, r, w, z, c_grid_ks_np,\n",
        "            b_grid_t.cpu().numpy(), y_grid_t.cpu().numpy(), z_grid_t.cpu().numpy(),\n",
        "            r_grid_t.cpu().numpy(), w_grid_t.cpu().numpy(), ny_spg)\n",
        "    print(\"Testing trained policy:\")\n",
        "    b_test = np.array([1.0, 5.0, 10.0])\n",
        "    y_test = y_grid[1]\n",
        "    r_test, w_test = (r_min + r_max) / 2.0, (w_min + w_max) / 2.0\n",
        "    z_test = z_grid[len(z_grid)//2]\n",
        "    c_test, n_test, b_next_test = policy_trained_ks(b_test, y_test, r_test, w_test, z_test)\n",
        "    print(f\"  b = {b_test}, y = {y_test:.3f}, r = {r_test:.3f}, w = {w_test:.3f}, z = {z_test:.3f}\")\n",
        "    print(f\"  c = {c_test}, n = {n_test}, b_next = {b_next_test}\")\n",
        "except NameError:\n",
        "    print('Run the SPG training cell above first to define theta.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb192607",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "- **Model:** Krusell–Smith (1998): (c, n) = π(b, y, r, w); budget c + b' = (1+r)b + w·n·y; **total labour L = 1**. Prices (r, w) = P*(K, z) with L=1.\n",
        "- **State dynamics:** y, z from Ty, Tz (same as Huggett).\n",
        "- **Policy:** Raw (c, b') from θ; transition uses soft weights in `update_d_pi_direct_ks` (no full A matrix), same design as Huggett.\n",
        "- **SRL/SPG:** Two-phase training: warm-up with fixed d0 (steady-state under initial θ), then d evolves via direct map d_{t+1} = (implied A)@d_t. Gradient-stop on (r, w); L(θ) = E[Σ_t β^t d_t' u(c_t)]."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
