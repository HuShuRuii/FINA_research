# Huggett 模型训练逻辑说明

## 核心思想

我们训练一个**消费策略网格** `θ`，其中每个网格点都是一个可训练参数。通过梯度下降优化这些参数，使得期望生命周期效用最大化。

## 1. 网格参数化

### 1.1 状态空间

- **个体状态**: `(b, y)` - 债券持有量和异质性收入
- **聚合状态**: `z` - 总生产率
- **价格**: `r` - 利率（由市场出清决定）

### 1.2 网格结构

消费策略被参数化为一个三维网格 `θ[j, iz, ir]`：

- `j = ib * ny + iy`: 个体状态索引
  - `ib`: 债券网格索引 (0 到 `nb_spg-1`)
  - `iy`: 收入网格索引 (0 到 `ny-1`)
- `iz`: 聚合状态 `z` 的网格索引 (0 到 `nz_spg-1`)
- `ir`: 利率 `r` 的网格索引 (0 到 `nr_spg-1`)

**网格形状**: `(J, nz_spg, nr_spg)`，其中 `J = nb_spg * ny`

### 1.3 Softplus 变换

**Softplus 函数**是一个平滑的、可微的激活函数，定义为：

$$\text{softplus}(x) = \ln(1 + e^x)$$

**数学性质**：
- **值域**: `[0, +∞)` - 输出始终非负
- **单调性**: 严格单调递增
- **可微性**: 处处可微，导数为 `σ(x)`（sigmoid 函数）
- **平滑性**: 是 ReLU 的平滑版本

**为什么使用 Softplus**：

1. **确保非负性**: 消费 `c` 必须 ≥ 0，`softplus` 保证输出非负
2. **可微性**: 对 `θ` 可微，支持梯度下降
3. **平滑性**: 比硬约束（如 `max(θ, 0)`）更平滑，训练更稳定
4. **灵活性**: 允许 `θ` 取任意实数值，通过变换自动满足约束

**在代码中的应用**：

```python
def theta_to_consumption_grid(theta, ...):
    """将 theta 转换为消费网格"""
    # softplus(theta) 确保 >= 0
    # + c_min_val 确保 >= c_min (如 0.001)
    return torch.nn.functional.softplus(theta) + c_min_val
```

**示例**：
- 如果 `θ[j, iz, ir] = -5` → `softplus(-5) ≈ 0.007` → `c ≈ 0.007 + 0.001 = 0.008`
- 如果 `θ[j, iz, ir] = 0` → `softplus(0) ≈ 0.693` → `c ≈ 0.693 + 0.001 = 0.694`
- 如果 `θ[j, iz, ir] = 5` → `softplus(5) ≈ 5.007` → `c ≈ 5.007 + 0.001 = 5.008`

### 1.4 消费策略获取

对于任意状态 `(b, y, r, z)`：

1. 找到对应的网格索引 `(ib, iy, iz, ir)`
2. 计算状态索引 `j = ib * ny + iy`
3. 从网格中获取参数值：`θ[j, iz, ir]`
4. 通过 `softplus` 变换得到消费：`c = softplus(θ[j, iz, ir]) + c_min`

```python
def theta_to_consumption_grid(theta, ...):
    """将 theta 转换为消费网格"""
    return torch.nn.functional.softplus(theta) + c_min_val
```

## 2. 训练目标

### 2.1 目标函数

最大化期望生命周期效用：

$$L(\theta) = \frac{1}{N} \sum_{n=1}^{N} \sum_{t=0}^{T} \beta^t G_t^T u(c_t)$$

其中：
- `N`: 模拟轨迹数量
- `T`: 时间步长
- `β`: 贴现因子
- **`G_t`**: 时刻 `t` 的**状态分布**（fraction of states），即各状态上代理人的比例，满足 $\sum_j G_t[j]=1$
- `u(c_t)`: 效用函数（CRRA）
- `c_t`: 从策略网格 `θ` 中获取的消费

**G 的演化（不是简单相加）**：由策略 π 得到转移矩阵 $A_\pi$，$A_\pi[j',j] = P(s'=j'|s=j)$，则
$$G_{t+1} = A_\pi(z_t, r_t) \, G_t$$
即用转移矩阵左乘当前分布得到下一期分布。

### 2.2 梯度停止（Gradient Stop）

**关键机制**: 对价格 `r_t` 应用梯度停止

- 价格 `r_t = P^*(G_t, z_t)` 由市场出清条件（给定当前分布 G_t）决定
- 使用 `.detach()` 阻止梯度通过价格传播
- **含义**: 个体将价格视为给定，不通过改变策略来影响价格

```python
def P_star_detach(theta, d, iz, ...):
    """找到市场出清的利率，并应用梯度停止"""
    r_t = find_market_clearing_rate(...)
    return r_t.detach()  # 梯度停止
```

## 3. 训练流程

### 3.1 初始化

```python
# 初始化网格参数 theta
theta = init_theta(b_grid_spg, y_grid_t, z_grid_t, r_grid_t)
theta = theta.requires_grad_(True)  # 启用梯度追踪
```

初始化策略：使用简单的储蓄规则（如储蓄 20% 的收入）来初始化消费网格。

### 3.2 训练循环

对每个 epoch：

1. **前向传播**:
   ```python
   L = spg_objective(theta, N_traj, T_horizon, ...)
   ```
   - 模拟 `N_traj` 条轨迹
   - 每条轨迹运行 `T_horizon` 步
   - 计算期望效用 `L(θ)`

2. **反向传播**:
   ```python
   (-L).backward()  # 最大化 L 等价于最小化 -L
   ```
   - 计算梯度 `∇_θ L(θ)`
   - 注意：价格 `r_t` 已应用梯度停止，梯度不会通过价格传播

3. **参数更新**:
   ```python
   optimizer.step()
   ```
   - 使用 Adam 优化器更新网格参数 `θ`

### 3.3 单条轨迹的计算

对于每条轨迹 `n`，**维护 G（fraction of states）**，并通过**由策略 π 得到的转移矩阵**更新（不是简单相加）：

```python
for t in range(T_horizon):
    # 1. 给定当前分布 G，计算市场出清价格（梯度停止）
    r_t = P_star_detach(theta, G, iz, ...)
    
    # 2. 从策略网格获取消费
    c = theta_to_consumption_grid(theta, ...)
    c_t = c[:, iz, ir]  # 当前状态下的消费向量
    
    # 3. 计算效用并累加：G^T u(c_t)
    L_n += β^t * (G @ u_torch(c_t))
    
    # 4. G 通过策略 π 的转移矩阵 A_π 演化（不是简单相加）
    A_pi = build_A_pi(theta, iz, ir, ...)  # A_π[j',j] = P(s'=j'|s=j)
    G = A_pi @ G  # G_{t+1} = A_π @ G_t
    
    # 5. 更新聚合状态 z
    iz = draw_next_z(iz, Tz)
```

## 4. 转移矩阵构建

### 4.1 状态转移

从状态 `j = (ib, iy)` 转移到 `j' = (ib', iy')`：

1. **债券转移**: 根据预算约束计算下一期债券
   ```python
   b_next = (1 + r) * b + y * z - c
   b_next = clamp(b_next, b_min, b_max)
   ```

2. **软权重映射**: 将连续值 `b_next` 映射到离散网格
   ```python
   w_b = exp(-(b_next - b_grid)^2 / (2*sigma^2))
   w_b = w_b / sum(w_b)  # 归一化
   ```

3. **转移概率**: 
   ```python
   A[j', j] = w_b[ib'] * Ty[iy, iy']
   ```
   - `w_b[ib']`: 债券转移到 `ib'` 的权重
   - `Ty[iy, iy']`: 收入从 `iy` 转移到 `iy'` 的概率

### 4.2 可微性

转移矩阵 `A_π` 对 `θ` 可微，因为：
- 消费 `c` 来自 `θ`（通过 `softplus`）
- `b_next` 依赖于 `c`
- 权重 `w_b` 依赖于 `b_next`
- 因此 `A_π` 对 `θ` 可微

## 5. 市场出清

### 5.1 聚合储蓄

聚合储蓄由当前分布 G 与各状态下的储蓄 b' 加权得到（G 为各状态占比）：

```python
def aggregate_saving_grid(theta, G, iz, ir, ...):
    """G = fraction of states. 聚合储蓄 = G^T b'(θ,z,r)"""
    c = theta_to_consumption_grid(theta, ...)[:, iz, ir]
    b_next = (1 + r) * b_grid + y_grid * z - c
    return (G * b_next).sum()  # 按分布 G 加权
```

### 5.2 价格确定

在 Huggett 模型中，债券净供给 `B = 0`，因此给定当前分布 G 和 z，找使聚合储蓄 = 0 的利率：

```python
def P_star_detach(theta, G, iz, ...):
    """给定 G_t, z_t，找到使聚合储蓄 = 0 的利率"""
    for ir in range(nr_spg):
        S = aggregate_saving_grid(theta, G, iz, ir, ...)
        if abs(S - 0) < best_err:
            best_ir = ir
    return r_grid[best_ir].detach()  # 梯度停止
```

## 6. 使用训练好的策略

### 6.1 转换网格

```python
# 训练完成后
theta_trained = theta.detach()
c_grid = theta_to_consumption_grid(theta_trained, ...)
c_grid_np = c_grid.cpu().numpy()
```

### 6.2 策略函数

```python
def policy_trained(b, y, r, z):
    """使用训练好的网格获取消费策略"""
    return policy_from_grid(
        b, y, r, z,
        c_grid_np,  # 训练好的消费网格
        b_grid_spg, y_grid_t, z_grid_t, r_grid_t,
        ny
    )
```

### 6.3 预算约束

```python
# 从网格获取消费
c = policy_from_grid(...)

# 计算下一期债券
c_total = (1 + r) * b + y * z  # 总资源
b_next = c_total - c  # 剩余资源
b_next = clamp(b_next, b_min, b_max)  # 限制在允许范围内

# 如果 b_next 被裁剪，调整消费以满足预算
c = max(c_total - b_next, c_min)
```

## 7. 关键特点总结

1. **网格参数化**: 每个网格点 `θ[j, iz, ir]` 都是一个可训练参数
2. **梯度下降**: 使用 Adam 优化器更新所有网格参数
3. **梯度停止**: 价格 `r_t` 不参与梯度传播（个体视价格为给定）
4. **可微转移**: 状态转移矩阵对策略参数可微
5. **市场出清**: 价格由市场出清条件内生决定（但梯度停止）

## 8. 训练参数

- **网格大小**: `nb_spg=50, ny=3, nz_spg=10, nr_spg=10`
- **总参数数**: `J * nz_spg * nr_spg = 50*3 * 10 * 10 = 15,000`
- **优化器**: Adam, `lr=1e-3`
- **训练设置**: `N_traj=32, T_horizon=30, n_epochs=100`

## 9. 与固定规则的区别

**固定规则**（如 `save_frac=0.2`）:
- 消费 = `(1 - save_frac) * 总收入`
- 不随状态变化优化

**训练网格**:
- 消费 = `θ[j, iz, ir]`（从网格中查找）
- 通过梯度下降优化，最大化期望效用
- 能够学习到状态依赖的最优策略
